// TODO: LangChain integration - Implement later when needed
/*
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
  AIMessagePromptTemplate,
} from "langchain/prompts";
import { StringOutputParser } from "langchain/schema/output_parser";
*/
import { ChatMessage } from "../types";

export class LangChainService {
  // TODO: LangChain implementation
  // private model: ChatOpenAI;

  constructor() {
    console.log("‚ö†Ô∏è LangChain service initialized but not implemented yet");

    // TODO: Initialize LangChain components
    /*
    this.model = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: process.env.OPENAI_MODEL || "gpt-3.5-turbo",
      temperature: 0.7,
      maxTokens: 2000,
    });
    */
  }

  async generateResponse(
    message: string,
    context: ChatMessage[],
    systemPrompt?: string,
    options?: {
      model?: string;
      temperature?: number;
      maxTokens?: number;
    }
  ): Promise<string> {
    // TODO: Implement LangChain integration
    console.log("üîÑ Simulating AI response generation...");

    // Temporary mock response
    return `Xin ch√†o! T√¥i ƒë√£ nh·∫≠n ƒë∆∞·ª£c tin nh·∫Øn: "${message}". 
ƒê√¢y l√† response gi·∫£ l·∫≠p. LangChain integration s·∫Ω ƒë∆∞·ª£c implement sau.
Context messages: ${context.length}`;

    /* TODO: Implement actual LangChain logic
    try {
      // Create custom model if options provided
      const chatModel = options
        ? new ChatOpenAI({
            openAIApiKey: process.env.OPENAI_API_KEY,
            modelName:
              options.model || process.env.OPENAI_MODEL || "gpt-3.5-turbo",
            temperature: options.temperature ?? 0.7,
            maxTokens: options.maxTokens ?? 2000,
          })
        : this.model;

      // Build prompt template
      const messages = [];

      // Add system message
      const defaultSystemPrompt = `B·∫°n l√† AI Learning Assistant cho Deep Knowledge AI Platform. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
- H·ªó tr·ª£ h·ªçc vi√™n hi·ªÉu s√¢u v·ªÅ ch·ªß ƒë·ªÅ ƒëang h·ªçc
- ƒê∆∞a ra gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu v·ªõi v√≠ d·ª• th·ª±c t·∫ø
- Khuy·∫øn kh√≠ch t∆∞ duy ph·∫£n bi·ªán v√† ƒë·∫∑t c√¢u h·ªèi
- ƒêi·ªÅu ch·ªânh phong c√°ch gi·∫£ng d·∫°y theo tr√¨nh ƒë·ªô h·ªçc vi√™n
- Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát

H√£y t∆∞∆°ng t√°c m·ªôt c√°ch th√¢n thi·ªán, ƒë·ªông vi√™n v√† h·ªó tr·ª£ t·ªët nh·∫•t cho h·ªçc vi√™n.`;

      messages.push(
        SystemMessagePromptTemplate.fromTemplate(
          systemPrompt || defaultSystemPrompt
        )
      );

      // Add context messages
      for (const msg of context) {
        if (msg.role === "user") {
          messages.push(HumanMessagePromptTemplate.fromTemplate(msg.content));
        } else if (msg.role === "assistant") {
          messages.push(AIMessagePromptTemplate.fromTemplate(msg.content));
        }
      }

      // Add current message
      messages.push(HumanMessagePromptTemplate.fromTemplate(message));

      // Create chat prompt
      const chatPrompt = ChatPromptTemplate.fromMessages(messages);

      // Create chain
      const chain = chatPrompt.pipe(chatModel).pipe(new StringOutputParser());

      // Generate response
      const response = await chain.invoke({});

      return response;
    } catch (error) {
      console.error("LangChain error:", error);
      throw new Error("Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi t·ª´ AI");
    }
    */
  }

  async generateSummary(messages: ChatMessage[]): Promise<string> {
    // TODO: Implement summary generation with LangChain
    console.log("üîÑ Simulating summary generation...");

    return `T√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i (${messages.length} tin nh·∫Øn): 
ƒê√¢y l√† summary gi·∫£ l·∫≠p. LangChain integration s·∫Ω ƒë∆∞·ª£c implement sau.`;

    /* TODO: Implement actual summary logic
    try {
      const summaryPrompt = ChatPromptTemplate.fromTemplate(`
H√£y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i sau ƒë√¢y m·ªôt c√°ch ng·∫Øn g·ªçn, n√™u r√µ:
1. Ch·ªß ƒë·ªÅ ch√≠nh ƒëang th·∫£o lu·∫≠n
2. Nh·ªØng ƒëi·ªÉm quan tr·ªçng ƒë√£ ƒë∆∞·ª£c trao ƒë·ªïi
3. Ki·∫øn th·ª©c ch√≠nh m√† h·ªçc vi√™n ƒë√£ h·ªçc ƒë∆∞·ª£c

Cu·ªôc h·ªôi tho·∫°i:
{conversation}

T√≥m t·∫Øt (t·ªëi ƒëa 200 t·ª´):
`);

      const conversation = messages
        .map(
          (msg) => `${msg.role === "user" ? "H·ªçc vi√™n" : "AI"}: ${msg.content}`
        )
        .join("\n");

      const chain = summaryPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser());

      const summary = await chain.invoke({ conversation });

      return summary;
    } catch (error) {
      console.error("Summary generation error:", error);
      throw new Error("Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i");
    }
    */
  }

  async analyzeTopicProgress(
    topicDescription: string,
    nodeDescriptions: string[],
    completedNodes: string[]
  ): Promise<{
    overallProgress: number;
    suggestions: string[];
    nextSteps: string[];
  }> {
    // TODO: Implement progress analysis with LangChain
    console.log("üîÑ Simulating progress analysis...");

    // Mock analysis
    const progress = Math.round(
      (completedNodes.length / nodeDescriptions.length) * 100
    );

    return {
      overallProgress: progress,
      suggestions: [
        "ƒê√¢y l√† ph√¢n t√≠ch gi·∫£ l·∫≠p",
        "LangChain s·∫Ω ƒë∆∞·ª£c implement sau",
        "Hi·ªán t·∫°i ch·ªâ t√≠nh progress c∆° b·∫£n",
      ],
      nextSteps: [
        "Implement LangChain integration",
        "Setup OpenAI API",
        "Test v·ªõi real data",
      ],
    };

    /* TODO: Implement actual analysis logic
    try {
      const analysisPrompt = ChatPromptTemplate.fromTemplate(`
Ph√¢n t√≠ch ti·∫øn ƒë·ªô h·ªçc t·∫≠p:

Ch·ªß ƒë·ªÅ: {topicDescription}

T·∫•t c·∫£ c√°c nodes:
{allNodes}

Nodes ƒë√£ ho√†n th√†nh:
{completedNodes}

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON v·ªõi format:
{{
  "overallProgress": <s·ªë t·ª´ 0-100>,
  "suggestions": ["g·ª£i √Ω 1", "g·ª£i √Ω 2", ...],
  "nextSteps": ["b∆∞·ªõc ti·∫øp theo 1", "b∆∞·ªõc ti·∫øp theo 2", ...]
}}
`);

      const chain = analysisPrompt
        .pipe(this.model)
        .pipe(new StringOutputParser());

      const result = await chain.invoke({
        topicDescription,
        allNodes: nodeDescriptions.join("\n"),
        completedNodes: completedNodes.join("\n"),
      });

      try {
        return JSON.parse(result);
      } catch (parseError) {
        console.error("Failed to parse analysis result:", parseError);
        return {
          overallProgress: Math.round(
            (completedNodes.length / nodeDescriptions.length) * 100
          ),
          suggestions: ["Ti·∫øp t·ª•c h·ªçc c√°c node ti·∫øp theo"],
          nextSteps: ["Ho√†n th√†nh c√°c node c√≤n l·∫°i"],
        };
      }
    } catch (error) {
      console.error("Topic analysis error:", error);
      throw new Error("Kh√¥ng th·ªÉ ph√¢n t√≠ch ti·∫øn ƒë·ªô h·ªçc t·∫≠p");
    }
    */
  }
}

export const langChainService = new LangChainService();
