"""
ğŸ”¥ Test LangChain OpenRouter vá»›i STREAMING
Test streaming functionality vá»›i LLMConfig Ä‘Ã£ Ä‘Æ°á»£c update
"""

import asyncio
from dotenv import load_dotenv
from app.models.llm_config import LLMConfig
from langchain_core.messages import HumanMessage

load_dotenv()

async def test_streaming_chat():
    """Test streaming chat vá»›i OpenRouter"""
    print("ğŸš€ Testing LangChain Streaming with OpenRouter")
    print("=" * 60)
    
    try:
        # Táº¡o LLM vá»›i streaming enabled (disable retry wrapper)
        llm = LLMConfig.get_llm(
            model_name="google/gemini-2.0-flash-lite-001",  # Fast model
            streaming=True,  # Force streaming
            temperature=0.7,
            max_tokens=100,
            enable_retry=False  # Disable retry wrapper to avoid field modification
        )
        
        print(f"âœ… LLM created successfully")
        print(f"ğŸ“¡ Streaming enabled: {llm.streaming}")
        
        # Test basic message
        messages = [
            HumanMessage(content="ChÃ o báº¡n! HÃ£y tráº£ lá»i ngáº¯n gá»n báº±ng tiáº¿ng Viá»‡t.")
        ]
        
        print("\nğŸ¯ Sending message (streaming)...")
        
        # Stream response tá»«ng chunk
        response_chunks = []
        async for chunk in llm.astream(messages):
            if hasattr(chunk, 'content') and chunk.content:
                print(f"ğŸ“¥ Chunk: '{chunk.content}'")
                response_chunks.append(chunk.content)
        
        full_response = "".join(response_chunks)
        print(f"\nâœ… Complete response: '{full_response}'")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_batch_streaming():
    """Test batch processing vá»›i streaming"""
    print("\n" + "=" * 60)
    print("ğŸ”„ Testing Batch Streaming")
    
    try:
        llm = LLMConfig.get_llm(
            model_name="google/gemini-2.0-flash-lite-001",
            streaming=True,
            temperature=0.3,
            enable_retry=False  # Disable retry wrapper
        )
        
        # Multiple questions
        questions = [
            "Äá»‹nh nghÄ©a AI trong 1 cÃ¢u",
            "Machine Learning lÃ  gÃ¬?", 
            "Blockchain hoáº¡t Ä‘á»™ng ra sao?"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ Question {i}: {question}")
            
            response_parts = []
            async for chunk in llm.astream([HumanMessage(content=question)]):
                if hasattr(chunk, 'content') and chunk.content:
                    response_parts.append(chunk.content)
            
            answer = "".join(response_parts)
            print(f"ğŸ’¡ Answer: {answer}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Batch error: {e}")
        return False

if __name__ == "__main__":
    async def main():
        success1 = await test_streaming_chat()
        success2 = await test_batch_streaming()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š STREAMING TEST RESULTS:")
        print(f"   Basic streaming: {'âœ… PASS' if success1 else 'âŒ FAIL'}")
        print(f"   Batch streaming: {'âœ… PASS' if success2 else 'âŒ FAIL'}")
        
        if success1 and success2:
            print("ğŸ‰ All streaming tests PASSED!")
            print("ğŸ’¡ OpenRouter streaming integration working correctly")
        else:
            print("âš ï¸  Some tests failed - check logs above")
    
    asyncio.run(main()) 